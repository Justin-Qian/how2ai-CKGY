import torch
from transformers import LayoutLMv3Processor, LayoutLMv3Model
from PIL import Image
from typing import List, Tuple

# Assuming config.py is in the same directory or accessible
try:
    import config
except ImportError:
    # Fallback or default values if config.py is not found
    # This might happen if running the script from a different context
    print("Warning: config.py not found. Using default model name.")
    config = type('obj', (object,), {'LAYOUTLM_MODEL_NAME': 'microsoft/layoutlmv3-base', 'LAYOUTLM_IMAGE_SIZE': (224, 224)})()


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

_processor = None
_model = None

def load_layoutlm_model_and_processor():
    """Loads the LayoutLMv3 model and processor."""
    global _processor, _model
    if _processor is None or _model is None:
        print(f"Loading LayoutLMv3 model and processor: {config.LAYOUTLM_MODEL_NAME}")
        _processor = LayoutLMv3Processor.from_pretrained(config.LAYOUTLM_MODEL_NAME, apply_ocr=False)
        _model = LayoutLMv3Model.from_pretrained(config.LAYOUTLM_MODEL_NAME)
        _model.to(device)
        _model.eval() # Set model to evaluation mode
        print("LayoutLMv3 model and processor loaded.")
    return _processor, _model

def normalize_bbox(bbox: Tuple[float, float, float, float], page_width: float, page_height: float) -> List[int]:
    """Normalizes bounding box coordinates to the 0-1000 range expected by LayoutLM."""
    x0, y0, x1, y1 = bbox
    return [
        int(1000 * (x0 / page_width)),
        int(1000 * (y0 / page_height)),
        int(1000 * (x1 / page_width)),
        int(1000 * (y1 / page_height)),
    ]

def preprocess_image_for_layoutlm(image: Image.Image) -> Image.Image:
    """Preprocesses the image for LayoutLMv3 input (resizing)."""
    # LayoutLMv3 processor handles normalization internally if pixel_values are not provided directly.
    # We just need to ensure the image is in RGB and potentially resize if needed,
    # although the processor can often handle resizing too. Let's resize explicitly based on config.
    image = image.convert("RGB")
    image = image.resize(config.LAYOUTLM_IMAGE_SIZE)
    return image

def extract_layoutlm_features(
    page_image: Image.Image,
    words: List[str],
    boxes: List[Tuple[float, float, float, float]], # Original PDF coordinates
    page_dims: Tuple[float, float] # page_width, page_height
) -> Tuple[List[int], torch.Tensor]:
    """
    Extracts token IDs and hidden state embeddings using LayoutLMv3.

    Args:
        page_image: PIL Image of the page.
        words: List of words extracted from the page.
        boxes: List of bounding boxes (original PDF coordinates) corresponding to words.
        page_dims: Tuple containing (page_width, page_height).

    Returns:
        A tuple containing:
        - input_ids: List of token IDs generated by the processor.
        - last_hidden_state: Tensor of shape (batch_size, sequence_length, hidden_size)
                           containing the embeddings from the last layer.
    """
    processor, model = load_layoutlm_model_and_processor()
    page_width, page_height = page_dims

    # Preprocess image
    processed_image = preprocess_image_for_layoutlm(page_image)

    # Normalize bounding boxes
    normalized_boxes = [normalize_bbox(box, page_width, page_height) for box in boxes]

    # Ensure words and boxes lists are not empty
    if not words or not boxes:
        print("Warning: Empty words or boxes list provided. Skipping LayoutLM processing for this page/section.")
        return [], torch.empty((0, 0, model.config.hidden_size), device=device)


    # Process with LayoutLMv3Processor
    # Note: The processor expects words and boxes for layout information.
    # It handles tokenization and alignment internally.
    encoding = processor(
        processed_image,
        words,
        boxes=normalized_boxes,
        return_tensors="pt",
        truncation=True, # Truncate if sequence exceeds max length
        padding="max_length", # Pad to max length
        max_length=512 # Default max length for LayoutLMv3
    ).to(device)

    # Perform inference
    with torch.no_grad():
        outputs = model(**encoding)

    last_hidden_state = outputs.last_hidden_state
    input_ids = encoding.input_ids.squeeze().tolist() # Get input IDs as a list

    return input_ids, last_hidden_state

# Example usage (for testing purposes)
if __name__ == '__main__':
    # This part will only run when the script is executed directly
    # You would need a sample image, words, and boxes to test
    print("LayoutLM Utils - Example Usage (requires test data)")
    # processor, model = load_layoutlm_model_and_processor()
    # print(f"Model hidden size: {model.config.hidden_size}")
    # Create dummy data
    # dummy_image = Image.new('RGB', (1000, 1000), color = 'white')
    # dummy_words = ["Hello", "world"]
    # dummy_boxes = [(100, 100, 200, 120), (210, 100, 300, 120)]
    # page_dims = (1000, 1000)
    # input_ids, embeddings = extract_layoutlm_features(dummy_image, dummy_words, dummy_boxes, page_dims)
    # print(f"Input IDs length: {len(input_ids)}")
    # print(f"Embeddings shape: {embeddings.shape}")
